---
title: "Replication of Social Intuition: Behavioral and Neurological Considerations by Jellema et al. (2024, Frontiers in Psychology)"
author: "Wilder Hartwell (wphartwell@ucsd.edu"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

**Link to experiment:** <https://psyc-201.github.io/jellema2024/jellema2024_16.html>

This paper was one of the inspirations for my first year project. It includes a social implicit learning paradigm, which is what I am doing in my first year project. As the title of the paper suggests, it looks at social intuition, which is my central research interest. The study is also done in neurotypical participants but administers the autism quotient to look at correlations between autistic traits and other variables of interest. This makes the study more practical for an in class replication on a short timeline. Another benefit of this study is that it doesn’t use any skills that are too far out of my reach, but still presents some opportunities for growth.

In the acquisition phase, face morphs of dynamic facial expressions (happy or angry) in which their gaze shifts towards or away from the participant will be shown to participants across multiple trials. All faces are either identity A or B. Identity A and B are showing either a positive or negative disposition towards the participant. In the test phase, implicit learning was tested via morphs of the facial expressions of both identities put together, progressively shifting towards more of one identity than the other, while participants indicated whether the identity was closer to A or B. The smiling and the frowning faces were tested separately. The idea was that if the participant had learned the disposition of the two identities, they would be able to indicate whether the face was more A or B.

I will give 56 trials split into two blocks of 28 trials in the acquisition phase. The response in the test phase will ask participants to press "f" or "j" to indicate that they think the face is more identity A or B after viewing the video clip on each trial. In the nonsocial task, participants will do the same thing except identities A and B are actually circle vs. square.

The test phase will be four blocks of ten trials. It starts with 60% maximally smiling agent A and 40% maximally smiling agent B and progresses in steps of 5% towards 60% maximally smiling agent B, which only takes 5 trials. I'd like to give participants two rounds of the smiling trials. Then the same thing is repeated for the frowning faces, and I would give two rounds of that as well. Their accuracy scores in the test phase will serve as the measure of their implicit learning. At the end, participants will be asked questions probing whether they detected the contingencies in the experiment and if they did their data will be excluded.

In the original experiment, there is also a nonsocial task, but that will not be replicated due to feasibility on a very short timeline.

I attempted to get the face stimuli from the authors but was not successful.

## Methods

35 adult participants were recruited for this study. The study was run on Prolific and participants were compensated for their time. The experiment took approximately 10-15 minutes to complete. For the face stimuli, photos were taken and then manipulated to create Identity B. The experiment was coded in jsPsych.

There were two blocks of 28 trials in the acquisition phase. In the acquisition phase, the happy face condition started with gaze forward, smiling and gradually shifted to gaze away from the participant, frowning. The angry condition started with gaze forward, frowning and gradually shifted to gaze away from the participant, smiling. The clips were 2s long.

There were four blocks of 10 trials in the test phase. The test phase began with a combined face of 60% maximally smiling agent A and 40% maximally smiling agent B and progressed in steps of 5% towards 60% maximally smiling agent B. Participants saw this four times. Then the same thing was repeated for the frowning faces, and the participants saw this four times as well. At the end, participants were asked questions probing whether they detected the contingencies in the experiment and if they did their data was excluded.

### Power Analysis

A power analysis run using an ANOVA in RStudio using the smallest original main effect of ηp\^2 = 0.12 (disposition) found that I would need 30 participants to achieve a power of 80%. Thus, I planned to recruit 35 participants to account for not all data being usable. This is a feasible number of participants to recruit with the resources available.

### Analysis Plan

### 

---
title: "StatsReplicationOfJellema2024"
format: html
editor: visual
---

**Load packages**

```{r install/loading packages}
library(tidyverse)
library(ggplot2) # plotting
library(ggthemes) # good for making plots pretty
library(effectsize)
library(knitr)
library(kableExtra)
library(purrr)
library(tidyr)
library(ggpubr)

```

**Import csv and transpose**

```{r import csv and transpose}
#import csv
data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_data.csv", header = FALSE)
#data <- read.csv(file = "/Users/wilderhartwell/Documents/PSYCH201/PilotA_Replication.csv", header = FALSE)
                 
#make tibble
data <- tibble(data)

data <- data %>%
  mutate(across(everything(), ~na_if(.x, ""))) %>%
  mutate(across(everything(), ~na_if(.x, " "))) %>%
  mutate(across(everything(), ~na_if(.x, "NA"))) %>%
  filter(!if_all(everything(), is.na))

data_long <- as.data.frame(t(data))

# Rename columns
colnames(data_long) <- as.character(unlist(data_long[1, ]))  # make first row the column names
data_long <- data_long[-1, ] 

```

```{r replace strings with numeric}
# Recode words into numeric values
data_long <- data_long |>
  mutate(Q1_num = case_when(
      Q1 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q1 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
    TRUE ~ NA_real_ # fallback for anything else 
    )) |>
    mutate(Q2_num = case_when(
      Q2 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q2 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q3_num = case_when(
      Q3 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q3 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q4_num = case_when(
      Q4 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q4 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q5_num = case_when(
      Q5 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q5 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q6_num = case_when(
      Q6 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q6 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q7_num = case_when(
      Q7 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q7 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q8_num = case_when(
      Q8 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q8 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q9_num = case_when(
      Q9 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q9 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q10_num = case_when(
      Q10 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q10 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ1_num = case_when(
      CheckQ1 %in% c("I do not know") ~ 0,
      CheckQ1 %in% c("Negative") ~ 1,
      CheckQ1 %in% c("Neutral") ~ 2,
      CheckQ1 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ2_num = case_when(
      CheckQ2 %in% c("I do not know") ~ 0,
      CheckQ2 %in% c("Negative") ~ 1,
      CheckQ2 %in% c("Neutral") ~ 2,
      CheckQ2 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_num = case_when(
      frown50_1 %in% c("f") ~ 0,
      frown50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_num = case_when(
      frown50_2 %in% c("f") ~ 0,
      frown50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_num = case_when(
      smile50_1 %in% c("f") ~ 0,
      smile50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_num = case_when(
      smile50_2 %in% c("f") ~ 0,
      smile50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_cor_num = case_when(
      frown50_1_cor %in% c("FALSE") ~ 0,
      frown50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_cor_num = case_when(
      frown50_2_cor %in% c("FALSE") ~ 0,
      frown50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_cor_num = case_when(
      smile50_1_cor %in% c("FALSE") ~ 0,
      smile50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_cor_num = case_when(
      smile50_2_cor %in% c("FALSE") ~ 0,
      smile50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(gender_num = case_when(
      Q_gender %in% c("Woman") ~ 0,
      Q_gender %in% c("Man") ~ 1,
      Q_gender %in% c("Nonbinary/other") ~ 2,
      TRUE ~ NA_real_
    )) |>
    mutate(age_num = case_when(
      Q_age %in% c("18-24") ~ 0,
      Q_age %in% c("25-34") ~ 1,
      Q_age %in% c("35-44") ~ 2,
      Q_age %in% c("45-54") ~ 3,
      Q_age %in% c("55-64") ~ 4,
      Q_age %in% c("65+") ~ 5,
      TRUE ~ NA_real_
    ))
```

**Prepare the non-ANOVA spreadsheet**

```{r calculate scores}

#the difference between the frown and smile at 50/50 (used in correlation)
FrownScore <- data_long$frown50_1_cor_num + data_long$frown50_2_cor_num
FrownScore
SmileScore <- data_long$smile50_1_cor_num + data_long$smile50_2_cor_num
SmileScore

TestScore <- FrownScore - SmileScore
TestScore <- abs(TestScore)

data_long <- bind_cols(data_long, TestScore)

attach(data_long)
#Score the AQ-10
AQScore <- rowSums(data_long[, c("Q1_num","Q2_num", "Q3_num", "Q4_num", "Q5_num", "Q6_num", "Q7_num", "Q8_num", "Q9_num", "Q10_num")])
as.data.frame(AQScore)
data_long <- bind_cols(data_long, AQScore)

#make tibble again
data_long <- tibble(data_long)

```

**Exclude participants**

```{r Exclude participants}
# ---------------------------------------------------------
# 0. Total participants
# ---------------------------------------------------------
n_total <- n_distinct(data_long$ID)


# ---------------------------------------------------------
# 1. Exclusion A — Detected contingencies
# ---------------------------------------------------------
excluded_detection <- data_long %>%
  filter(CheckQ1 == "Positive" & CheckQ2 == "Negative") %>%
  pull(ID)


# ---------------------------------------------------------
# 2. Exclusion B — Missing all 50/50 responses
# ---------------------------------------------------------
excluded_missing <- data_long %>%
  filter(is.na(frown50_1_num) & 
         is.na(smile50_1_num) & 
         is.na(frown50_2_num) & 
         is.na(smile50_2_num)) %>%
  pull(ID)


# Collect initial exclusion IDs
excluded_initial <- unique(c(excluded_detection, excluded_missing))

# Remove these from main dataset before further checks
data_complete <- data_long %>%
  filter(!(ID %in% excluded_initial))


# ---------------------------------------------------------
# 3. RT & accuracy exclusion rules
# ---------------------------------------------------------

rt_attention_cols  <- c("rt_attention_1", "rt_attention_2", "rt_attention_3", "rt_attention_4")
acc_attention_cols <- c("attention_1",   "attention_2",   "attention_3",   "attention_4")
rt_test_cols       <- c("rt_frown_1", "rt_smile_1", "rt_frown_2", "rt_smile_2")

# Convert columns safely
data_complete <- data_complete %>%
  mutate(across(all_of(c(rt_attention_cols, rt_test_cols)), ~ as.numeric(as.character(.))),
         across(all_of(acc_attention_cols), ~ as.integer(as.character(.))))

data_complete <- data_complete %>%
  mutate(
    fail_attention_rt  = if_any(all_of(rt_attention_cols),  ~ .x > 11000),
    fail_attention_acc = if_any(all_of(acc_attention_cols), ~ .x == 0),
    fail_test_rt       = if_any(all_of(rt_test_cols),       ~ (.x <= 900 | .x > 30000))
  )


# ---------------------------------------------------------
# 4. Build a MULTI-REASON exclusion table
# ---------------------------------------------------------

# Helper: convert logical flags into reason labels
reason_map <- c(
  fail_detection      = "Detected Contingencies",
  fail_missing        = "Missing All 50/50 Trials",
  fail_attention_rt   = "Attention RT > 11000",
  fail_attention_acc  = "Incorrect Attention Check",
  fail_test_rt        = "Test Trial RT Out of Range"
)

# Initial exclusion table
initial_df <- tibble(
    ID = excluded_initial,
    fail_detection = excluded_initial %in% excluded_detection,
    fail_missing   = excluded_initial %in% excluded_missing
) %>%
    pivot_longer(
        cols = starts_with("fail_"),
        names_to = "flag",
        values_to = "value"
    ) %>%
    filter(value) %>%
    mutate(Reason = reason_map[flag] %||% NA_character_) %>%
    select(ID, Reason)

# DIAGNOSTIC — must be separate
data_complete %>%
  select(ID, starts_with("fail_")) %>%
  pivot_longer(cols = starts_with("fail_"),
               names_to = "flag", values_to = "value") %>%
  count(flag, value)

  
# Build long-format table of later RT/ACC exclusions
late_df <- data_complete %>%
  select(ID, starts_with("fail_")) %>%
  distinct() %>%
  pivot_longer(
    cols      = starts_with("fail_"),
    names_to  = "flag",
    values_to = "value"
  ) %>%
  filter(value == TRUE) %>%
  mutate(
    Reason = dplyr::recode(flag, !!!reason_map, .default = NA_character_)
  ) %>%
  select(ID, Reason)



# Combine ALL reasons
excluded_df_long <- bind_rows(initial_df, late_df) %>%
  distinct()

# Save excluded list (LONG FORMAT)
write.csv(excluded_df_long, "excluded_participants_long.csv", row.names = FALSE)


# ---------------------------------------------------------
# 6. Final cleaned dataset
# ---------------------------------------------------------
all_excluded_ids <- unique(excluded_df_long$ID)

data_clean <- data_long %>%
  filter(!(ID %in% all_excluded_ids))


# ---------------------------------------------------------
# 7. Print summary
# ---------------------------------------------------------
cat("======== EXCLUSION SUMMARY ========\n")
cat("Total participants:", n_total, "\n")
cat("Excluded:", length(all_excluded_ids), "\n")
cat("Remaining:", n_distinct(data_clean$ID), "\n\n")

cat("Sample of multi-reason exclusions:\n")
print(head(excluded_df_long))

```

**Demographics**

```{r demographics}
# Age distribution
ggplot(data_complete, aes(x = age_num)) +
  geom_histogram(binwidth = 1, fill = "#1976D2", alpha = 0.7) +
  labs(title = "Age Distribution",
       x = "Age",
       y = "Count")

# Age summary
data_complete %>%
  summarise(
    mean_age = mean(age_num, na.rm = TRUE),
    sd_age = sd(age_num, na.rm = TRUE),
    min_age = min(age_num, na.rm = TRUE),
    max_age = max(age_num, na.rm = TRUE)
  ) %>%
  kable(digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

# Gender distribution
ggplot(data_complete, aes(x = gender_num)) +
  geom_histogram(binwidth = 1, fill = "#1976D2", alpha = 0.7) +
  labs(title = "Gender Distribution",
       x = "Gender",
       y = "Count")
```

**Mean score**

```{r mean score}
acc_mean <- mean(TestScore)
acc_mean
```

**Plotting data**

```{r accuracy plot}
library(ggplot2)

ggplot(data_complete, aes(x = TestScore)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Test Scores",
    x = "Test Score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14)
```

**Open spreadsheet formatted for the ANOVA**

This spreadsheet is constructed outside of R without the excluded participants

```{r import ANOVA spreadsheet}
A_data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_ANOVA_data.csv")

A_data <- A_data |>
  mutate(Response_num = case_when(
      Response %in% c("f") ~ 0,
      Response %in% c("j") ~ 1,
    TRUE ~ NA_real_ # fallback for anything else 
    ))
```

**Run ANOVA**

```{r ANOVA}
#run two-way repeated measures ANOVA using aov
model.aov <- aov(
  Response_num ~ Disposition * Proportion +
    Error(ID / (Disposition * Proportion)),
  data = A_data
)
summary(model.aov)

lapply(A_data[, c("Disposition","Proportion","ID")], function(x) {
  list(
    class = class(x),
    has_na = any(is.na(x)),
    has_blank = any(x == "" )
  )
})


effectsize(model.aov)
```

**Run correlation**

```{r correlation}

cor.test(AQScore, TestScore)
```

```{r plot correlation}

ggplot(data_complete, aes(x = AQScore, y = TestScore)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  stat_cor(method = "pearson", label.x = Inf, label.y = Inf, hjust = 1.1, vjust = 1.5) +
  theme_classic() +
  labs(
    x = "AQ Score",
    y = "Test Score",
    title = "Correlation Between AQScore and TestScore"
  )

```

## Results

### Data preparation

To prepare the data, I used RStudio. Full data preparation code is shown below.

#### **Load packages**

```{r install/loading packages - data prep}
library(tidyverse)
library(ggplot2) # plotting
library(ggthemes) # good for making plots pretty
library(effectsize)
library(knitr)
library(kableExtra)
library(purrr)
library(tidyr)
library(ggpubr)

```

**Import csv and transpose**

```{r import csv and transpose - data prep}
#import csv
data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_data.csv", header = FALSE)
#data <- read.csv(file = "/Users/wilderhartwell/Documents/PSYCH201/PilotA_Replication.csv", header = FALSE)
                 
#make tibble
data <- tibble(data)

data <- data %>%
  mutate(across(everything(), ~na_if(.x, ""))) %>%
  mutate(across(everything(), ~na_if(.x, " "))) %>%
  mutate(across(everything(), ~na_if(.x, "NA"))) %>%
  filter(!if_all(everything(), is.na))

data_long <- as.data.frame(t(data))

# Rename columns
colnames(data_long) <- as.character(unlist(data_long[1, ]))  # make first row the column names
data_long <- data_long[-1, ] 

```

```{r replace strings with numeric - data prep}
# Recode words into numeric values
data_long <- data_long |>
  mutate(Q1_num = case_when(
      Q1 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q1 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
    TRUE ~ NA_real_ # fallback for anything else 
    )) |>
    mutate(Q2_num = case_when(
      Q2 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q2 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q3_num = case_when(
      Q3 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q3 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q4_num = case_when(
      Q4 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q4 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q5_num = case_when(
      Q5 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q5 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q6_num = case_when(
      Q6 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q6 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q7_num = case_when(
      Q7 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q7 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q8_num = case_when(
      Q8 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q8 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q9_num = case_when(
      Q9 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q9 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q10_num = case_when(
      Q10 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q10 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ1_num = case_when(
      CheckQ1 %in% c("I do not know") ~ 0,
      CheckQ1 %in% c("Negative") ~ 1,
      CheckQ1 %in% c("Neutral") ~ 2,
      CheckQ1 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ2_num = case_when(
      CheckQ2 %in% c("I do not know") ~ 0,
      CheckQ2 %in% c("Negative") ~ 1,
      CheckQ2 %in% c("Neutral") ~ 2,
      CheckQ2 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_num = case_when(
      frown50_1 %in% c("f") ~ 0,
      frown50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_num = case_when(
      frown50_2 %in% c("f") ~ 0,
      frown50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_num = case_when(
      smile50_1 %in% c("f") ~ 0,
      smile50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_num = case_when(
      smile50_2 %in% c("f") ~ 0,
      smile50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_cor_num = case_when(
      frown50_1_cor %in% c("FALSE") ~ 0,
      frown50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_cor_num = case_when(
      frown50_2_cor %in% c("FALSE") ~ 0,
      frown50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_cor_num = case_when(
      smile50_1_cor %in% c("FALSE") ~ 0,
      smile50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_cor_num = case_when(
      smile50_2_cor %in% c("FALSE") ~ 0,
      smile50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(gender_num = case_when(
      Q_gender %in% c("Woman") ~ 0,
      Q_gender %in% c("Man") ~ 1,
      Q_gender %in% c("Nonbinary/other") ~ 2,
      TRUE ~ NA_real_
    )) |>
    mutate(age_num = case_when(
      Q_age %in% c("18-24") ~ 0,
      Q_age %in% c("25-34") ~ 1,
      Q_age %in% c("35-44") ~ 2,
      Q_age %in% c("45-54") ~ 3,
      Q_age %in% c("55-64") ~ 4,
      Q_age %in% c("65+") ~ 5,
      TRUE ~ NA_real_
    ))
```

**Prepare the non-ANOVA spreadsheet**

```{r calculate scores - data prep}

#the difference between the frown and smile at 50/50 (used in correlation)
FrownScore <- data_long$frown50_1_cor_num + data_long$frown50_2_cor_num
FrownScore
SmileScore <- data_long$smile50_1_cor_num + data_long$smile50_2_cor_num
SmileScore

TestScore <- FrownScore - SmileScore
TestScore <- abs(TestScore)

data_long <- bind_cols(data_long, TestScore)

attach(data_long)
#Score the AQ-10
AQScore <- rowSums(data_long[, c("Q1_num","Q2_num", "Q3_num", "Q4_num", "Q5_num", "Q6_num", "Q7_num", "Q8_num", "Q9_num", "Q10_num")])
as.data.frame(AQScore)
data_long <- bind_cols(data_long, AQScore)

#make tibble again
data_long <- tibble(data_long)

```

**Exclude participants**

```{r Exclude participants - data prep}
# ---------------------------------------------------------
# 0. Total participants
# ---------------------------------------------------------
n_total <- n_distinct(data_long$ID)


# ---------------------------------------------------------
# 1. Exclusion A — Detected contingencies
# ---------------------------------------------------------
excluded_detection <- data_long %>%
  filter(CheckQ1 == "Positive" & CheckQ2 == "Negative") %>%
  pull(ID)


# ---------------------------------------------------------
# 2. Exclusion B — Missing all 50/50 responses
# ---------------------------------------------------------
excluded_missing <- data_long %>%
  filter(is.na(frown50_1_num) & 
         is.na(smile50_1_num) & 
         is.na(frown50_2_num) & 
         is.na(smile50_2_num)) %>%
  pull(ID)


# Collect initial exclusion IDs
excluded_initial <- unique(c(excluded_detection, excluded_missing))

# Remove these from main dataset before further checks
data_complete <- data_long %>%
  filter(!(ID %in% excluded_initial))


# ---------------------------------------------------------
# 3. RT & accuracy exclusion rules
# ---------------------------------------------------------

rt_attention_cols  <- c("rt_attention_1", "rt_attention_2", "rt_attention_3", "rt_attention_4")
acc_attention_cols <- c("attention_1",   "attention_2",   "attention_3",   "attention_4")
rt_test_cols       <- c("rt_frown_1", "rt_smile_1", "rt_frown_2", "rt_smile_2")

# Convert columns safely
data_complete <- data_complete %>%
  mutate(across(all_of(c(rt_attention_cols, rt_test_cols)), ~ as.numeric(as.character(.))),
         across(all_of(acc_attention_cols), ~ as.integer(as.character(.))))

data_complete <- data_complete %>%
  mutate(
    fail_attention_rt  = if_any(all_of(rt_attention_cols),  ~ .x > 11000),
    fail_attention_acc = if_any(all_of(acc_attention_cols), ~ .x == 0),
    fail_test_rt       = if_any(all_of(rt_test_cols),       ~ (.x <= 900 | .x > 30000))
  )


# ---------------------------------------------------------
# 4. Build a MULTI-REASON exclusion table
# ---------------------------------------------------------

# Helper: convert logical flags into reason labels
reason_map <- c(
  fail_detection      = "Detected Contingencies",
  fail_missing        = "Missing All 50/50 Trials",
  fail_attention_rt   = "Attention RT > 11000",
  fail_attention_acc  = "Incorrect Attention Check",
  fail_test_rt        = "Test Trial RT Out of Range"
)

# Initial exclusion table
initial_df <- tibble(
    ID = excluded_initial,
    fail_detection = excluded_initial %in% excluded_detection,
    fail_missing   = excluded_initial %in% excluded_missing
) %>%
    pivot_longer(
        cols = starts_with("fail_"),
        names_to = "flag",
        values_to = "value"
    ) %>%
    filter(value) %>%
    mutate(Reason = reason_map[flag] %||% NA_character_) %>%
    select(ID, Reason)

# DIAGNOSTIC — must be separate
data_complete %>%
  select(ID, starts_with("fail_")) %>%
  pivot_longer(cols = starts_with("fail_"),
               names_to = "flag", values_to = "value") %>%
  count(flag, value)

  
# Build long-format table of later RT/ACC exclusions
late_df <- data_complete %>%
  select(ID, starts_with("fail_")) %>%
  distinct() %>%
  pivot_longer(
    cols      = starts_with("fail_"),
    names_to  = "flag",
    values_to = "value"
  ) %>%
  filter(value == TRUE) %>%
  mutate(
    Reason = dplyr::recode(flag, !!!reason_map, .default = NA_character_)
  ) %>%
  select(ID, Reason)



# Combine ALL reasons
excluded_df_long <- bind_rows(initial_df, late_df) %>%
  distinct()

# Save excluded list (LONG FORMAT)
write.csv(excluded_df_long, "excluded_participants_long.csv", row.names = FALSE)


# ---------------------------------------------------------
# 6. Final cleaned dataset
# ---------------------------------------------------------
all_excluded_ids <- unique(excluded_df_long$ID)

data_clean <- data_long %>%
  filter(!(ID %in% all_excluded_ids))


# ---------------------------------------------------------
# 7. Print summary
# ---------------------------------------------------------
cat("======== EXCLUSION SUMMARY ========\n")
cat("Total participants:", n_total, "\n")
cat("Excluded:", length(all_excluded_ids), "\n")
cat("Remaining:", n_distinct(data_clean$ID), "\n\n")

cat("Sample of multi-reason exclusions:\n")
print(head(excluded_df_long))

```

### Confirmatory analysis

A two-way repeated measures ANOVA with DISPOSITION (2 levels) and PROPORTION (5 levels) as factors was performed on participant responses to the social task. This test was used because I needed to compare the means of multiple groups to see if there was a significant difference between them. A correlation was tested between the questionnaire on autistic traits and responses to the social task. This correlation test was completed because I was looking at the potential connection between autistic traits and performance on the task.
