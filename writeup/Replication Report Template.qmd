---
title: "Replication of Study X by Sample & Sample (20xx, Psychological Science)"
author: "Replication Author[s] (contact information)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

\[No abstract is needed.\] Each replication project will have a straightforward, no frills report of the study and results. These reports will be publicly available as supplementary material for the aggregate report(s) of the project as a whole. Also, to maximize project integrity, the intro and methods will be written and critiqued in advance of data collection. Introductions can be just 1-2 paragraphs clarifying the main idea of the original study, the target finding for replication, and any other essential information. You will likely contact the authors of the original study, so this should be professional and forward facing. It will NOT have a literature review -- that is in the original publication. You can write both the introduction and the methods in past tense.

**Link to experiment:** <https://psyc-201.github.io/jellema2024/jellema2024_16.html>

This paper was one of the inspirations for my first year project. It includes a social implicit learning paradigm, which is what I am doing in my first year project. As the title of the paper suggests, it looks at social intuition, which is my central research interest. The study is also done in neurotypical participants but administers the autism quotient to look at correlations between autistic traits and other variables of interest. This makes the study more practical for an in class replication on a short timeline. I plan to get the face morphs used as stimuli for the replication in order to make this project much easier. I have some concerns about whether I will be able to get the stimuli since Leslie thinks someone else in our lab may have asked them for their stimuli and never got a response. If I cannot get the face stimuli, I will probably have to pivot, but I've sent out the email request and we'll see what happens. Another benefit of this study is that it doesn’t use any skills that are too far out of my reach, but still presents some opportunities for growth.

In the acquisition phase, face morphs of dynamic facial expressions (happy or angry) in which their gaze shifts towards or away from the participant will be shown to participants across multiple trials. All faces are either identity A or B. Identity A and B are showing either a positive or negative disposition towards the participant. In the test phase, implicit learning was tested via morphs of the facial expressions of both identities put together, progressively shifting towards more of one identity than the other, while participants indicated whether the identity was closer to A or B. The smiling and the frowning faces were tested separately. The idea was that if the participant had learned the disposition of the two identities, they would be able to indicate whether the face was more A or B.

I will give 56 trials split into two blocks of 28 trials in the acquisition phase. The response in the test phase will ask participants to press "f" or "j" to indicate that they think the face is more identity A or B after viewing the video clip on each trial. In the nonsocial task, participants will do the same thing except identities A and B are actually circle vs. square.

The test phase will be four blocks of ten trials. It starts with 60% maximally smiling agent A and 40% maximally smiling agent B and progresses in steps of 5% towards 60% maximally smiling agent B, which only takes 5 trials. I'd like to give participants two rounds of the smiling trials. Then the same thing is repeated for the frowning faces, and I would give two rounds of that as well. Their accuracy scores in the test phase will serve as the measure of their implicit learning. At the end, participants will be asked questions probing whether they detected the contingencies in the experiment and if they did their data will be excluded.

In the original experiment, there is also a nonsocial task, but that will not be replicated due to feasibility on a very short timeline.

I attempted to get the face stimuli from the authors but was not successful. Thus the remaining tasks are to create the sitmuli and code the experiment to run the face stimuli correctly. I would likely use jspsych for that, which I anticipate will pose slight challenges but nothing I can’t overcome, especially with the support offered in class.

Do we need to add to the introduction?

## Methods

35 adult participants were recruited for this study. The study was run on Prolific and participants were compensated for their time. The experiment took approximately 10-15 minutes to complete. Google Gemini was used to generate the face morph stimuli. The experiment was coded in JsPsych.

There were two blocks of 28 trials in the acquisition phase. In the acquisition phase, the happy face condition started with gaze forward, smiling and gradually shifted to gaze away from the participant, frowning. The angry condition started with gaze forward, frowning and gradually shifted to gaze away from the participant, smiling. The clips were 2s long.

There were four blocks of 10 trials in the test phase. The test phase began with a combined face of 60% maximally smiling agent A and 40% maximally smiling agent B and progressed in steps of 5% towards 60% maximally smiling agent B. Participants saw this four times. Then the same thing was repeated for the frowning faces, and the participants saw this four times as well. At the end, participants were asked questions probing whether they detected the contingencies in the experiment and if they did their data was excluded.

### Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size. Considerations of feasibility for selecting planned sample size.

A power analysis run using an ANOVA in RStudio using the smallest original main effect of ηp\^2 = 0.12 (disposition) found that I would need 30 participants to achieve a power of 80%. Thus, I planned to recruit 35 participants to account for not all data being usable. This is a feasible number of participants to recruit with the resources available.

### Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

### Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely. Or, quote directly and just point out exceptions to what was described in the original article.

### Procedure

Can quote directly from original article - just put the text in quotations and note that this was followed precisely. Or, quote directly and just point out exceptions to what was described in the original article.

### Analysis Plan

---
title: "StatsReplicationOfJellema2024"
format: html
editor: visual
---

**Load packages**

```{r install/loading packages}
library(tidyverse)
library(ggplot2) # plotting
library(ggthemes) # good for making plots pretty
library(effectsize)
library(knitr)
library(kableExtra)
library(purrr)
library(tidyr)
library(ggpubr)

```

**Import csv and transpose**

```{r import csv and transpose}
#import csv
data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_data.csv", header = FALSE)
#data <- read.csv(file = "/Users/wilderhartwell/Documents/PSYCH201/PilotA_Replication.csv", header = FALSE)
                 
#make tibble
data <- tibble(data)

data <- data %>%
  mutate(across(everything(), ~na_if(.x, ""))) %>%
  mutate(across(everything(), ~na_if(.x, " "))) %>%
  mutate(across(everything(), ~na_if(.x, "NA"))) %>%
  filter(!if_all(everything(), is.na))

data_long <- as.data.frame(t(data))

# Rename columns
colnames(data_long) <- as.character(unlist(data_long[1, ]))  # make first row the column names
data_long <- data_long[-1, ] 

```

```{r replace strings with numeric}
# Recode words into numeric values
data_long <- data_long |>
  mutate(Q1_num = case_when(
      Q1 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q1 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
    TRUE ~ NA_real_ # fallback for anything else 
    )) |>
    mutate(Q2_num = case_when(
      Q2 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q2 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q3_num = case_when(
      Q3 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q3 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q4_num = case_when(
      Q4 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q4 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q5_num = case_when(
      Q5 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q5 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q6_num = case_when(
      Q6 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q6 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q7_num = case_when(
      Q7 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q7 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q8_num = case_when(
      Q8 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q8 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q9_num = case_when(
      Q9 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q9 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q10_num = case_when(
      Q10 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q10 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ1_num = case_when(
      CheckQ1 %in% c("I do not know") ~ 0,
      CheckQ1 %in% c("Negative") ~ 1,
      CheckQ1 %in% c("Neutral") ~ 2,
      CheckQ1 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ2_num = case_when(
      CheckQ2 %in% c("I do not know") ~ 0,
      CheckQ2 %in% c("Negative") ~ 1,
      CheckQ2 %in% c("Neutral") ~ 2,
      CheckQ2 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_num = case_when(
      frown50_1 %in% c("f") ~ 0,
      frown50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_num = case_when(
      frown50_2 %in% c("f") ~ 0,
      frown50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_num = case_when(
      smile50_1 %in% c("f") ~ 0,
      smile50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_num = case_when(
      smile50_2 %in% c("f") ~ 0,
      smile50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_cor_num = case_when(
      frown50_1_cor %in% c("FALSE") ~ 0,
      frown50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_cor_num = case_when(
      frown50_2_cor %in% c("FALSE") ~ 0,
      frown50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_cor_num = case_when(
      smile50_1_cor %in% c("FALSE") ~ 0,
      smile50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_cor_num = case_when(
      smile50_2_cor %in% c("FALSE") ~ 0,
      smile50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(gender_num = case_when(
      Q_gender %in% c("Woman") ~ 0,
      Q_gender %in% c("Man") ~ 1,
      Q_gender %in% c("Nonbinary/other") ~ 2,
      TRUE ~ NA_real_
    )) |>
    mutate(age_num = case_when(
      Q_age %in% c("18-24") ~ 0,
      Q_age %in% c("25-34") ~ 1,
      Q_age %in% c("35-44") ~ 2,
      Q_age %in% c("45-54") ~ 3,
      Q_age %in% c("55-64") ~ 4,
      Q_age %in% c("65+") ~ 5,
      TRUE ~ NA_real_
    ))
```

**Prepare the non-ANOVA spreadsheet**

```{r calculate scores}

#the difference between the frown and smile at 50/50 (used in correlation)
FrownScore <- data_long$frown50_1_cor_num + data_long$frown50_2_cor_num
FrownScore
SmileScore <- data_long$smile50_1_cor_num + data_long$smile50_2_cor_num
SmileScore

TestScore <- FrownScore - SmileScore
TestScore <- abs(TestScore)

data_long <- bind_cols(data_long, TestScore)

attach(data_long)
#Score the AQ-10
AQScore <- rowSums(data_long[, c("Q1_num","Q2_num", "Q3_num", "Q4_num", "Q5_num", "Q6_num", "Q7_num", "Q8_num", "Q9_num", "Q10_num")])
as.data.frame(AQScore)
data_long <- bind_cols(data_long, AQScore)

#make tibble again
data_long <- tibble(data_long)

```

**Exclude participants**

```{r Exclude participants}
# ---------------------------------------------------------
# 0. Total participants
# ---------------------------------------------------------
n_total <- n_distinct(data_long$ID)


# ---------------------------------------------------------
# 1. Exclusion A — Detected contingencies
# ---------------------------------------------------------
excluded_detection <- data_long %>%
  filter(CheckQ1 == "Positive" & CheckQ2 == "Negative") %>%
  pull(ID)


# ---------------------------------------------------------
# 2. Exclusion B — Missing all 50/50 responses
# ---------------------------------------------------------
excluded_missing <- data_long %>%
  filter(is.na(frown50_1_num) & 
         is.na(smile50_1_num) & 
         is.na(frown50_2_num) & 
         is.na(smile50_2_num)) %>%
  pull(ID)


# Collect initial exclusion IDs
excluded_initial <- unique(c(excluded_detection, excluded_missing))

# Remove these from main dataset before further checks
data_complete <- data_long %>%
  filter(!(ID %in% excluded_initial))


# ---------------------------------------------------------
# 3. RT & accuracy exclusion rules
# ---------------------------------------------------------

rt_attention_cols  <- c("rt_attention_1", "rt_attention_2", "rt_attention_3", "rt_attention_4")
acc_attention_cols <- c("attention_1",   "attention_2",   "attention_3",   "attention_4")
rt_test_cols       <- c("rt_frown_1", "rt_smile_1", "rt_frown_2", "rt_smile_2")

# Convert columns safely
data_complete <- data_complete %>%
  mutate(across(all_of(c(rt_attention_cols, rt_test_cols)), ~ as.numeric(as.character(.))),
         across(all_of(acc_attention_cols), ~ as.integer(as.character(.))))

data_complete <- data_complete %>%
  mutate(
    fail_attention_rt  = if_any(all_of(rt_attention_cols),  ~ .x > 11000),
    fail_attention_acc = if_any(all_of(acc_attention_cols), ~ .x == 0),
    fail_test_rt       = if_any(all_of(rt_test_cols),       ~ (.x <= 900 | .x > 30000))
  )


# ---------------------------------------------------------
# 4. Build a MULTI-REASON exclusion table
# ---------------------------------------------------------

# Helper: convert logical flags into reason labels
reason_map <- c(
  fail_detection      = "Detected Contingencies",
  fail_missing        = "Missing All 50/50 Trials",
  fail_attention_rt   = "Attention RT > 11000",
  fail_attention_acc  = "Incorrect Attention Check",
  fail_test_rt        = "Test Trial RT Out of Range"
)

# Initial exclusion table
initial_df <- tibble(
    ID = excluded_initial,
    fail_detection = excluded_initial %in% excluded_detection,
    fail_missing   = excluded_initial %in% excluded_missing
) %>%
    pivot_longer(
        cols = starts_with("fail_"),
        names_to = "flag",
        values_to = "value"
    ) %>%
    filter(value) %>%
    mutate(Reason = reason_map[flag] %||% NA_character_) %>%
    select(ID, Reason)

# DIAGNOSTIC — must be separate
data_complete %>%
  select(ID, starts_with("fail_")) %>%
  pivot_longer(cols = starts_with("fail_"),
               names_to = "flag", values_to = "value") %>%
  count(flag, value)

  
# Build long-format table of later RT/ACC exclusions
late_df <- data_complete %>%
  select(ID, starts_with("fail_")) %>%
  distinct() %>%
  pivot_longer(
    cols      = starts_with("fail_"),
    names_to  = "flag",
    values_to = "value"
  ) %>%
  filter(value == TRUE) %>%
  mutate(
    Reason = dplyr::recode(flag, !!!reason_map, .default = NA_character_)
  ) %>%
  select(ID, Reason)



# Combine ALL reasons
excluded_df_long <- bind_rows(initial_df, late_df) %>%
  distinct()

# Save excluded list (LONG FORMAT)
write.csv(excluded_df_long, "excluded_participants_long.csv", row.names = FALSE)


# ---------------------------------------------------------
# 6. Final cleaned dataset
# ---------------------------------------------------------
all_excluded_ids <- unique(excluded_df_long$ID)

data_clean <- data_long %>%
  filter(!(ID %in% all_excluded_ids))


# ---------------------------------------------------------
# 7. Print summary
# ---------------------------------------------------------
cat("======== EXCLUSION SUMMARY ========\n")
cat("Total participants:", n_total, "\n")
cat("Excluded:", length(all_excluded_ids), "\n")
cat("Remaining:", n_distinct(data_clean$ID), "\n\n")

cat("Sample of multi-reason exclusions:\n")
print(head(excluded_df_long))

```

**Demographics**

```{r demographics}
# Age distribution
ggplot(data_complete, aes(x = age_num)) +
  geom_histogram(binwidth = 1, fill = "#1976D2", alpha = 0.7) +
  labs(title = "Age Distribution",
       x = "Age",
       y = "Count")

# Age summary
data_complete %>%
  summarise(
    mean_age = mean(age_num, na.rm = TRUE),
    sd_age = sd(age_num, na.rm = TRUE),
    min_age = min(age_num, na.rm = TRUE),
    max_age = max(age_num, na.rm = TRUE)
  ) %>%
  kable(digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

# Gender distribution
ggplot(data_complete, aes(x = gender_num)) +
  geom_histogram(binwidth = 1, fill = "#1976D2", alpha = 0.7) +
  labs(title = "Gender Distribution",
       x = "Gender",
       y = "Count")
```

**Mean score**

```{r mean score}
acc_mean <- mean(TestScore)
acc_mean
```

**Plotting data**

```{r accuracy plot}
library(ggplot2)

ggplot(data_complete, aes(x = TestScore)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "black", alpha = 0.7) +
  labs(
    title = "Distribution of Test Scores",
    x = "Test Score",
    y = "Count"
  ) +
  theme_minimal(base_size = 14)
```

**Open spreadsheet formatted for the ANOVA**

This spreadsheet is constructed outside of R without the excluded participants

```{r import ANOVA spreadsheet}
A_data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_ANOVA_data.csv")

A_data <- A_data |>
  mutate(Response_num = case_when(
      Response %in% c("f") ~ 0,
      Response %in% c("j") ~ 1,
    TRUE ~ NA_real_ # fallback for anything else 
    ))
```

**Run ANOVA**

```{r ANOVA}
#run two-way repeated measures ANOVA using aov
model.aov <- aov(
  Response_num ~ Disposition * Proportion +
    Error(ID / (Disposition * Proportion)),
  data = A_data
)
summary(model.aov)

lapply(A_data[, c("Disposition","Proportion","ID")], function(x) {
  list(
    class = class(x),
    has_na = any(is.na(x)),
    has_blank = any(x == "" )
  )
})


effectsize(model.aov)
```

**Run correlation**

```{r correlation}

cor.test(AQScore, TestScore)
```

```{r plot correlation}

ggplot(data_complete, aes(x = AQScore, y = TestScore)) +
  geom_point(alpha = 0.7) +
  geom_smooth(method = "lm", se = TRUE) +
  stat_cor(method = "pearson", label.x = Inf, label.y = Inf, hjust = 1.1, vjust = 1.5) +
  theme_classic() +
  labs(
    x = "AQ Score",
    y = "Test Score",
    title = "Correlation Between AQScore and TestScore"
  )

```

### Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study. The goal, of course, is to minimize those differences, but differences will inevitably occur. Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample

Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan

Any differences from what was described as the original plan, or “none”.

## Results

### Data preparation

To prepare the data, I used RStudio. Full data preparation code is shown below.

#### **Load packages**

```{r install/loading packages - data prep}
library(tidyverse)
library(ggplot2) # plotting
library(ggthemes) # good for making plots pretty
library(effectsize)
library(knitr)
library(kableExtra)
library(purrr)
library(tidyr)
library(ggpubr)

```

**Import csv and transpose**

```{r import csv and transpose - data prep}
#import csv
data <- read.csv(file = "/Users/wilderhartwell/Documents/jellema2024/data/Pilot_B/PilotB_data.csv", header = FALSE)
#data <- read.csv(file = "/Users/wilderhartwell/Documents/PSYCH201/PilotA_Replication.csv", header = FALSE)
                 
#make tibble
data <- tibble(data)

data <- data %>%
  mutate(across(everything(), ~na_if(.x, ""))) %>%
  mutate(across(everything(), ~na_if(.x, " "))) %>%
  mutate(across(everything(), ~na_if(.x, "NA"))) %>%
  filter(!if_all(everything(), is.na))

data_long <- as.data.frame(t(data))

# Rename columns
colnames(data_long) <- as.character(unlist(data_long[1, ]))  # make first row the column names
data_long <- data_long[-1, ] 

```

```{r replace strings with numeric - data prep}
# Recode words into numeric values
data_long <- data_long |>
  mutate(Q1_num = case_when(
      Q1 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q1 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
    TRUE ~ NA_real_ # fallback for anything else 
    )) |>
    mutate(Q2_num = case_when(
      Q2 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q2 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q3_num = case_when(
      Q3 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q3 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q4_num = case_when(
      Q4 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q4 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q5_num = case_when(
      Q5 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q5 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q6_num = case_when(
      Q6 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q6 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q7_num = case_when(
      Q7 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q7 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q8_num = case_when(
      Q8 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q8 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q9_num = case_when(
      Q9 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q9 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(Q10_num = case_when(
      Q10 %in% c("Slightly Agree", "Definitely Agree") ~ 1,
      Q10 %in% c("Slightly Disagree", "Definitely Disagree") ~ 0,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ1_num = case_when(
      CheckQ1 %in% c("I do not know") ~ 0,
      CheckQ1 %in% c("Negative") ~ 1,
      CheckQ1 %in% c("Neutral") ~ 2,
      CheckQ1 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(CheckQ2_num = case_when(
      CheckQ2 %in% c("I do not know") ~ 0,
      CheckQ2 %in% c("Negative") ~ 1,
      CheckQ2 %in% c("Neutral") ~ 2,
      CheckQ2 %in% c("Positive") ~ 3,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_num = case_when(
      frown50_1 %in% c("f") ~ 0,
      frown50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_num = case_when(
      frown50_2 %in% c("f") ~ 0,
      frown50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_num = case_when(
      smile50_1 %in% c("f") ~ 0,
      smile50_1 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_num = case_when(
      smile50_2 %in% c("f") ~ 0,
      smile50_2 %in% c("j") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_1_cor_num = case_when(
      frown50_1_cor %in% c("FALSE") ~ 0,
      frown50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(frown50_2_cor_num = case_when(
      frown50_2_cor %in% c("FALSE") ~ 0,
      frown50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_1_cor_num = case_when(
      smile50_1_cor %in% c("FALSE") ~ 0,
      smile50_1_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(smile50_2_cor_num = case_when(
      smile50_2_cor %in% c("FALSE") ~ 0,
      smile50_2_cor %in% c("TRUE") ~ 1,
      TRUE ~ NA_real_
    )) |>
    mutate(gender_num = case_when(
      Q_gender %in% c("Woman") ~ 0,
      Q_gender %in% c("Man") ~ 1,
      Q_gender %in% c("Nonbinary/other") ~ 2,
      TRUE ~ NA_real_
    )) |>
    mutate(age_num = case_when(
      Q_age %in% c("18-24") ~ 0,
      Q_age %in% c("25-34") ~ 1,
      Q_age %in% c("35-44") ~ 2,
      Q_age %in% c("45-54") ~ 3,
      Q_age %in% c("55-64") ~ 4,
      Q_age %in% c("65+") ~ 5,
      TRUE ~ NA_real_
    ))
```

**Prepare the non-ANOVA spreadsheet**

```{r calculate scores - data prep}

#the difference between the frown and smile at 50/50 (used in correlation)
FrownScore <- data_long$frown50_1_cor_num + data_long$frown50_2_cor_num
FrownScore
SmileScore <- data_long$smile50_1_cor_num + data_long$smile50_2_cor_num
SmileScore

TestScore <- FrownScore - SmileScore
TestScore <- abs(TestScore)

data_long <- bind_cols(data_long, TestScore)

attach(data_long)
#Score the AQ-10
AQScore <- rowSums(data_long[, c("Q1_num","Q2_num", "Q3_num", "Q4_num", "Q5_num", "Q6_num", "Q7_num", "Q8_num", "Q9_num", "Q10_num")])
as.data.frame(AQScore)
data_long <- bind_cols(data_long, AQScore)

#make tibble again
data_long <- tibble(data_long)

```

**Exclude participants**

```{r Exclude participants - data prep}
# ---------------------------------------------------------
# 0. Total participants
# ---------------------------------------------------------
n_total <- n_distinct(data_long$ID)


# ---------------------------------------------------------
# 1. Exclusion A — Detected contingencies
# ---------------------------------------------------------
excluded_detection <- data_long %>%
  filter(CheckQ1 == "Positive" & CheckQ2 == "Negative") %>%
  pull(ID)


# ---------------------------------------------------------
# 2. Exclusion B — Missing all 50/50 responses
# ---------------------------------------------------------
excluded_missing <- data_long %>%
  filter(is.na(frown50_1_num) & 
         is.na(smile50_1_num) & 
         is.na(frown50_2_num) & 
         is.na(smile50_2_num)) %>%
  pull(ID)


# Collect initial exclusion IDs
excluded_initial <- unique(c(excluded_detection, excluded_missing))

# Remove these from main dataset before further checks
data_complete <- data_long %>%
  filter(!(ID %in% excluded_initial))


# ---------------------------------------------------------
# 3. RT & accuracy exclusion rules
# ---------------------------------------------------------

rt_attention_cols  <- c("rt_attention_1", "rt_attention_2", "rt_attention_3", "rt_attention_4")
acc_attention_cols <- c("attention_1",   "attention_2",   "attention_3",   "attention_4")
rt_test_cols       <- c("rt_frown_1", "rt_smile_1", "rt_frown_2", "rt_smile_2")

# Convert columns safely
data_complete <- data_complete %>%
  mutate(across(all_of(c(rt_attention_cols, rt_test_cols)), ~ as.numeric(as.character(.))),
         across(all_of(acc_attention_cols), ~ as.integer(as.character(.))))

data_complete <- data_complete %>%
  mutate(
    fail_attention_rt  = if_any(all_of(rt_attention_cols),  ~ .x > 11000),
    fail_attention_acc = if_any(all_of(acc_attention_cols), ~ .x == 0),
    fail_test_rt       = if_any(all_of(rt_test_cols),       ~ (.x <= 900 | .x > 30000))
  )


# ---------------------------------------------------------
# 4. Build a MULTI-REASON exclusion table
# ---------------------------------------------------------

# Helper: convert logical flags into reason labels
reason_map <- c(
  fail_detection      = "Detected Contingencies",
  fail_missing        = "Missing All 50/50 Trials",
  fail_attention_rt   = "Attention RT > 11000",
  fail_attention_acc  = "Incorrect Attention Check",
  fail_test_rt        = "Test Trial RT Out of Range"
)

# Initial exclusion table
initial_df <- tibble(
    ID = excluded_initial,
    fail_detection = excluded_initial %in% excluded_detection,
    fail_missing   = excluded_initial %in% excluded_missing
) %>%
    pivot_longer(
        cols = starts_with("fail_"),
        names_to = "flag",
        values_to = "value"
    ) %>%
    filter(value) %>%
    mutate(Reason = reason_map[flag] %||% NA_character_) %>%
    select(ID, Reason)

# DIAGNOSTIC — must be separate
data_complete %>%
  select(ID, starts_with("fail_")) %>%
  pivot_longer(cols = starts_with("fail_"),
               names_to = "flag", values_to = "value") %>%
  count(flag, value)

  
# Build long-format table of later RT/ACC exclusions
late_df <- data_complete %>%
  select(ID, starts_with("fail_")) %>%
  distinct() %>%
  pivot_longer(
    cols      = starts_with("fail_"),
    names_to  = "flag",
    values_to = "value"
  ) %>%
  filter(value == TRUE) %>%
  mutate(
    Reason = dplyr::recode(flag, !!!reason_map, .default = NA_character_)
  ) %>%
  select(ID, Reason)



# Combine ALL reasons
excluded_df_long <- bind_rows(initial_df, late_df) %>%
  distinct()

# Save excluded list (LONG FORMAT)
write.csv(excluded_df_long, "excluded_participants_long.csv", row.names = FALSE)


# ---------------------------------------------------------
# 6. Final cleaned dataset
# ---------------------------------------------------------
all_excluded_ids <- unique(excluded_df_long$ID)

data_clean <- data_long %>%
  filter(!(ID %in% all_excluded_ids))


# ---------------------------------------------------------
# 7. Print summary
# ---------------------------------------------------------
cat("======== EXCLUSION SUMMARY ========\n")
cat("Total participants:", n_total, "\n")
cat("Excluded:", length(all_excluded_ids), "\n")
cat("Remaining:", n_distinct(data_clean$ID), "\n\n")

cat("Sample of multi-reason exclusions:\n")
print(head(excluded_df_long))

```

### Confirmatory analysis

The analyses as specified in the analysis plan.

A two-way repeated measures ANOVA with DISPOSITION (2 levels) and PROPORTION (5 levels) as factors was performed on participant responses to the social task. This test was used because I needed to compare the means of multiple groups to see if there was a significant difference between them. A correlation was tested between the questionnaire on autistic traits and responses to the social task. This correlation test was completed because I was looking at the potential connection between autistic traits and performance on the task.

On average, it took participants approximately 10 minutes to complete the study. This is different than the predicted 5-7 minutes to completion and thus the compensation amount needs to change to reflect this.

Pilot B data found that there was no significant main effect of Disposition (*F*(1,3) = 1.286, *p* = 0.339, *ηp2* = 0.30). There was also no main effect of Proportion (*F*(1,3) = 3, *p* = 0.182, *ηp2* = 0.50). There was no significant interaction effect (*F*(1,3) = 0, *p* = 1, *ηp2* = 0). Pilot B did not find a significant correlation between Autism Quotient-10 score and the difference between score on 50/50 frowning and smiling conditions (*r*(3) = -0.436, *p* = 0.693, two-tailed).

![](images/clipboard-1184043825.png)

### Exploratory analyses

Any follow-up analyses desired (not required).

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt. None of these need to be long.
